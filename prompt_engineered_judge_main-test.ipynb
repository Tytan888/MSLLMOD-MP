{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73340b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nah\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(\"nah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96d9f226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nah\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(\"nah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9056d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Final Score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tyra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Final Score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# get spearmans correlation bewteen Predicted_Score and Validation Score columns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m rho, pval \u001b[38;5;241m=\u001b[39m spearmanr(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted_Score\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinal Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpearman ρ = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrho\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tyra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Tyra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Final Score'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# get spearmans correlation bewteen Predicted_Score and Validation Score columns\n",
    "rho, pval = spearmanr(df[\"Predicted_Score\"], df[\"Validation Score\"])\n",
    "print(f\"Spearman ρ = {rho:.4f} (p={pval:.4g}) on {len(df)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f7c5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PROMPT = \"\"\"\n",
    "You are the Orchestration Layer of a Filipino-English LLM Judge system.\n",
    "\n",
    "Your job is to decide whether the given information (in JSON format) is enough to produce a final judgment on translation quality.\n",
    "{json}\n",
    "\n",
    "You must follow these decision rules:\n",
    "1. The fields \"english_sentence\" and \"filipino_sentence\" are absolutely essential. Do NOT proceed without them.\n",
    "2. Check if the system has already asked for the **context of the sentence** (in \"user_notes\").\n",
    "   - If YES, mark as complete.  \n",
    "   - If NO, add a step to ask the user for it.\n",
    "   Do not force the user to provide it.\n",
    "3. Check if the system has already asked for any **additional information** needed for judging (in \"user_notes\").  \n",
    "   - If YES, mark as complete.  \n",
    "   - If NO, add a step to ask the user for it.\n",
    "   Do not force the user to provide it.\n",
    "4. Determine if the system needs to use any tools to proceed. If YES, add those tool steps.  \n",
    "   If NO, list all tools that could still be used and their intended queries.\n",
    "\n",
    "Available tools and their command formats:\n",
    "- **Sentence similarity**: `similarity \"english phrase/sentence\" \"filipino phrase/sentence\"`\n",
    "- **English Dictionary lookup**: `lookup_english \"word\"`\n",
    "- **Filipino Dictionary lookup**: `lookup_filipino \"word\"`\n",
    "\n",
    "**Your output must be ONLY a JSON object in the following format:**\n",
    "{{\n",
    "  \"ready_to_decide\": true/false,\n",
    "  \"reason\": \"Why you can or cannot decide yet\",\n",
    "  \"steps\": [\n",
    "    \"ask \\\"...\\\"\",\n",
    "    \"similarity \\\"...\\\" \\\"...\\\"\",\n",
    "    \"lookup_english \\\"...\\\"\"\n",
    "    \"lookup_filipino \\\"...\\\"\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Where:\n",
    "- `\"ready_to_decide\"` is TRUE if all required info is present, otherwise FALSE.\n",
    "- `\"steps\"` is the ordered list of actions the system must take next.\n",
    "\n",
    "Return ONLY valid JSON. No extra text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "613bb982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please set GOOGLE_API_KEY in your environment.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "current_requests = 0\n",
    "rate_limit_num = 5\n",
    "rate_limit_wait = 1\n",
    "\n",
    "def call_gemini(prompt: str):\n",
    "    global current_requests\n",
    "    global rate_limit_num\n",
    "    global rate_limit_wait\n",
    "    if current_requests >= rate_limit_num:\n",
    "            print()\n",
    "            print(f\"Rate limit exceeded: {current_requests} > {rate_limit_num}\")\n",
    "            time.sleep(rate_limit_wait)\n",
    "            current_requests = 0\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL_NAME)\n",
    "        resp = model.generate_content(prompt)\n",
    "        return resp.text.strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Gemini call failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "55b9e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"english_sentence\": null,\n",
      "  \"filipino_sentence\": null,\n",
      "  \"accuracy_weight\": 1.5,\n",
      "  \"fluency_weight\": 1.5,\n",
      "  \"completeness_weight\": 0.5,\n",
      "  \"cultural_appropriateness_weight\": 0.5,\n",
      "  \"user_notes\": [],\n",
      "  \"tool_notes\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class TranslationJudgeMemory:\n",
    "    def __init__(self):\n",
    "        self.memory = {\n",
    "            \"english_sentence\": None,\n",
    "            \"filipino_sentence\": None,\n",
    "            \"accuracy_weight\": 1.5,\n",
    "            \"fluency_weight\": 1.5,\n",
    "            \"completeness_weight\": 0.5,\n",
    "            \"cultural_appropriateness_weight\": 0.5,\n",
    "            \"user_notes\": [],\n",
    "            \"tool_notes\": []\n",
    "        }\n",
    "\n",
    "    def update(self, key, value):\n",
    "        \"\"\"Update a memory field.\"\"\"\n",
    "        if key in self.memory:\n",
    "            self.memory[key] = value\n",
    "        else:\n",
    "            raise KeyError(f\"Memory key '{key}' not found.\")\n",
    "\n",
    "    def add_note(self, note, source=\"user\"):\n",
    "        \"\"\"Add note to either user_notes or tool_notes.\"\"\"\n",
    "        if source == \"user\":\n",
    "            self.memory[\"user_notes\"].append(note)\n",
    "        elif source == \"tool\":\n",
    "            self.memory[\"tool_notes\"].append(note)\n",
    "\n",
    "    def is_complete(self):\n",
    "        \"\"\"Check if we have enough info to proceed to scoring.\"\"\"\n",
    "        return all(self.memory[\"weights\"])  # simple completion check\n",
    "\n",
    "    def __str__(self):\n",
    "        return json.dumps(self.memory, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "test = TranslationJudgeMemory()\n",
    "# test.update(\"english_sentence\", \"The cat is on the roof.\")\n",
    "#test.update(\"filipino_sentence\", \"Ang pusa ay nasa bubong.\")\n",
    "#test.add_note(\"There is no context to this sentence. Just a simple statement about a cat on a roof.\", source=\"user\")\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cbd0978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are the Orchestration Layer of a Filipino-English LLM Judge system.\n",
      "\n",
      "Your job is to decide whether the given information (in JSON format) is enough to produce a final judgment on translation quality.\n",
      "{\n",
      "  \"english_sentence\": \"The cat is on the roof.\",\n",
      "  \"filipino_sentence\": \"Ang pusa ay nasa bubong.\",\n",
      "  \"accuracy_weight\": 1.5,\n",
      "  \"fluency_weight\": 1.5,\n",
      "  \"completeness_weight\": 0.5,\n",
      "  \"cultural_appropriateness_weight\": 0.5,\n",
      "  \"user_notes\": [\n",
      "    \"There is no context to this sentence. Just a simple statement about a cat on a roof.\"\n",
      "  ],\n",
      "  \"tool_notes\": []\n",
      "}\n",
      "\n",
      "You must follow these decision rules:\n",
      "1. The fields \"english_sentence\" and \"filipino_sentence\" are absolutely essential. Do NOT proceed without them.\n",
      "2. Check if the system has already asked for the **context of the sentence** (in \"user_notes\").\n",
      "   - If YES, mark as complete.  \n",
      "   - If NO, add a step to ask the user for it.\n",
      "   Do not force the user to provide it.\n",
      "3. Check if the system has already asked for any **additional information** needed for judging (in \"user_notes\").  \n",
      "   - If YES, mark as complete.  \n",
      "   - If NO, add a step to ask the user for it.\n",
      "   Do not force the user to provide it.\n",
      "4. Determine if the system needs to use any tools to proceed. If YES, add those tool steps.  \n",
      "   If NO, list all tools that could still be used and their intended queries.\n",
      "\n",
      "Available tools and their command formats:\n",
      "- **Sentence similarity**: `similarity \"english phrase/sentence\" \"filipino phrase/sentence\"`\n",
      "- **English Dictionary lookup**: `lookup_english \"word\"`\n",
      "- **Filipino Dictionary lookup**: `lookup_filipino \"word\"`\n",
      "\n",
      "**Your output must be ONLY a JSON object in the following format:**\n",
      "{\n",
      "  \"ready_to_decide\": true/false,\n",
      "  \"reason\": \"Why you can or cannot decide yet\",\n",
      "  \"steps\": [\n",
      "    \"ask \"...\"\",\n",
      "    \"similarity \"...\" \"...\"\",\n",
      "    \"lookup_english \"...\"\"\n",
      "    \"lookup_filipino \"...\"\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Where:\n",
      "- `\"ready_to_decide\"` is TRUE if all required info is present, otherwise FALSE.\n",
      "- `\"steps\"` is the ordered list of actions the system must take next.\n",
      "\n",
      "Return ONLY valid JSON. No extra text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = MAIN_PROMPT.format(json=test.__str__())\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9b73646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"ready_to_decide\": false,\n",
      "  \"reason\": \"Information is not sufficient to produce a final judgment. The system needs to ask for context and any additional information required for judging.\",\n",
      "  \"steps\": [\n",
      "    \"ask \\\"Please provide the context for this sentence, if any.\\\"\",\n",
      "    \"ask \\\"Please provide any additional information needed for judging.\\\"\",\n",
      "    \"similarity \\\"The cat is on the roof.\\\" \\\"Ang pusa ay nasa bubong.\\\"\",\n",
      "    \"lookup_english \\\"cat\\\"\",\n",
      "    \"lookup_english \\\"roof\\\"\",\n",
      "    \"lookup_filipino \\\"pusa\\\"\",\n",
      "    \"lookup_filipino \\\"bubong\\\"\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "res = call_gemini(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a6e6dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(text: str):\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    if start == -1 or end == -1:\n",
    "        raise ValueError(\"No JSON object found.\")\n",
    "    obj = json.loads(text[start:end+1])\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10f8a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = parse_json_response(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e6845283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ready_to_decide': False,\n",
       " 'reason': 'Information is not sufficient to produce a final judgment. The system needs to ask for context and any additional information required for judging.',\n",
       " 'steps': ['ask \"Please provide the context for this sentence, if any.\"',\n",
       "  'ask \"Please provide any additional information needed for judging.\"',\n",
       "  'similarity \"The cat is on the roof.\" \"Ang pusa ay nasa bubong.\"',\n",
       "  'lookup_english \"cat\"',\n",
       "  'lookup_english \"roof\"',\n",
       "  'lookup_filipino \"pusa\"',\n",
       "  'lookup_filipino \"bubong\"']}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b39f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to ask the user for:  Please provide the context for this sentence, if any.\n",
      "I need to ask the user for:  Please provide any additional information needed for judging.\n",
      "Need to check similarity between: ['The cat is on the roof.', 'Ang pusa ay nasa bubong.']\n",
      "Need to look up English word: cat\n",
      "Need to look up English word: roof\n",
      "Need to look up Filipino word: pusa\n",
      "Need to look up Filipino word: bubong\n"
     ]
    }
   ],
   "source": [
    "for step in res['steps']:\n",
    "    if step.startswith(\"ask\"):\n",
    "        question = step.split(\"\\\"\")[1]\n",
    "        print(\"I need to ask the user for: \", question)\n",
    "    elif step.startswith(\"similarity\"):\n",
    "        phrases = step.split(\"\\\"\")[1::2]\n",
    "        print(f\"Need to check similarity between: {phrases}\")\n",
    "    elif step.startswith(\"lookup_english\"):\n",
    "        word = step.split(\"\\\"\")[1]\n",
    "        print(f\"Need to look up English word: {word}\")\n",
    "    elif step.startswith(\"lookup_filipino\"):\n",
    "        word = step.split(\"\\\"\")[1]\n",
    "        print(f\"Need to look up Filipino word: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "372ded59",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_PROMPT = \"\"\"\n",
    "You are the Orchestration Layer of a Filipino-English LLM Judge system.\n",
    "\n",
    "Your job is to take in the user's input and extract the variables to be added to memory (in JSON format). The current memory is this:\n",
    "{json}\n",
    "\n",
    "Follow this format for adding to the memory and output lines based on what to add:\n",
    "1. If you see that the user has supplied the English or Filipino text, then add:\n",
    "english_sentence \"(English sentence)\"\n",
    "filipino_sentence \"(Filipino sentence)\"\n",
    "2. If the user has provided the context of the sentence, add it in the user notes:\n",
    "note \"(the context of the sentences)\"\n",
    "3. If the user has provided additional guidelines/instructions, then add it in the user notes:\n",
    "note \"(the additional instructions of the sentences)\"\n",
    "4. If the user has stated that no context or no additional instructions are to be provided, then add it in the user notes.\n",
    "\n",
    "**Your output must be ONLY a JSON object in the following format:**\n",
    "{{\n",
    "  \"updates\": [\n",
    "    \"english_sentence \\\"The cat is on the roof.\\\"\",\n",
    "    \"filipino_sentence \\\"Ang pusa ay nasa bubong.\\\"\",\n",
    "    \"note \\\"There is no context to this sentence. Just a simple statement about a cat on a roof.\\\"\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "The user has provided the following information:\n",
    "{input}\n",
    "\n",
    "Return ONLY valid JSON. No extra text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "06fb3a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are the Orchestration Layer of a Filipino-English LLM Judge system.\n",
      "\n",
      "Your job is to take in the user's input and extract the variables to be added to memory (in JSON format). The current memory is this:\n",
      "{\n",
      "  \"english_sentence\": null,\n",
      "  \"filipino_sentence\": null,\n",
      "  \"accuracy_weight\": 1.5,\n",
      "  \"fluency_weight\": 1.5,\n",
      "  \"completeness_weight\": 0.5,\n",
      "  \"cultural_appropriateness_weight\": 0.5,\n",
      "  \"user_notes\": [],\n",
      "  \"tool_notes\": []\n",
      "}\n",
      "\n",
      "Follow this format for adding to the memory and output lines based on what to add:\n",
      "1. If you see that the user has supplied the English or Filipino text, then add:\n",
      "english_sentence \"(English sentence)\"\n",
      "filipino_sentence \"(Filipino sentence)\"\n",
      "2. If the user has provided the context of the sentence, add it in the user notes:\n",
      "note \"(the context of the sentences)\"\n",
      "3. If the user has provided additional guidelines/instructions, then add it in the user notes:\n",
      "note \"(the additional instructions of the sentences)\"\n",
      "4. If the user has stated that no context or no additional instructions are to be provided, then add it in the user notes.\n",
      "\n",
      "**Your output must be ONLY a JSON object in the following format:**\n",
      "{\n",
      "  \"updates\": [\n",
      "    \"english_sentence \"The cat is on the roof.\"\",\n",
      "    \"filipino_sentence \"Ang pusa ay nasa bubong.\"\",\n",
      "    \"note \"There is no context to this sentence. Just a simple statement about a cat on a roof.\"\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "The user has provided the following information:\n",
      "English: The cat is on the roof. Filipino: Ang pusa ay nasa bubong. No additional context nor information.\n",
      "\n",
      "Return ONLY valid JSON. No extra text.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'updates': ['english_sentence \"The cat is on the roof.\"',\n",
       "  'filipino_sentence \"Ang pusa ay nasa bubong.\"',\n",
       "  'note \"No additional context nor information.\"']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update = UPDATE_PROMPT.format(json=test.__str__(), input=\"English: The cat is on the roof. Filipino: Ang pusa ay nasa bubong. No additional context nor information.\")\n",
    "print(update)\n",
    "res = call_gemini(update)\n",
    "res = parse_json_response(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdd526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To imply but stop short of saying explicitly.', 'To make one suppose; cause one to suppose (something).', 'To mention something as an idea, typically in order to recommend it', 'To seduce; to prompt to evil; to tempt.']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_english_definition(word):\n",
    "    url = f\"https://api.dictionaryapi.dev/api/v2/entries/en/{word}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        meanings = []\n",
    "        for meaning in data[0][\"meanings\"]:\n",
    "            for definition in meaning[\"definitions\"]:\n",
    "                meanings.append(definition[\"definition\"])\n",
    "        return meanings\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(get_english_definition(\"suggest\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43983643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.2622\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "\n",
    "# Load LaBSE model\n",
    "similarity_model_name = \"sentence-transformers/LaBSE\"\n",
    "similarity_tokenizer = AutoTokenizer.from_pretrained(similarity_model_name)\n",
    "similarity_model = AutoModel.from_pretrained(similarity_model_name)\n",
    "\n",
    "def labse_similarity(text1: str, text2: str) -> float:\n",
    "    # Tokenize\n",
    "    inputs = similarity_tokenizer([text1, text2], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = similarity_model(**inputs).pooler_output  # shape: (2, hidden_size)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Cosine similarity\n",
    "    similarity = torch.matmul(embeddings[0], embeddings[1]).item()\n",
    "    return similarity\n",
    "\n",
    "score = labse_similarity(\"\", \"Kamusta ka naman\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c3592eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiktionaryparser import WiktionaryParser\n",
    "\n",
    "# Initialize the parser once\n",
    "parser = WiktionaryParser()\n",
    "parser.set_default_language(\"english\")  # Retrieve Tagalog definitions from English Wiktionary\n",
    "\n",
    "def lookup_tagalog_word(word):\n",
    "    \"\"\"\n",
    "    Looks up a word on English Wiktionary and returns Tagalog-related definitions if available.\n",
    "    \"\"\"\n",
    "    entries = parser.fetch(word, \"english\")\n",
    "    if not entries:\n",
    "        return {\"word\": word, \"found\": False}\n",
    "\n",
    "    results = []\n",
    "    for entry in entries:\n",
    "        # Filter definitions that may involve Tagalog, or keep all\n",
    "        definitions = entry.get(\"definitions\", [])\n",
    "        if not definitions:\n",
    "            continue\n",
    "        for d in definitions:\n",
    "            text = \" \".join(d.get(\"text\", []))\n",
    "            pos = d.get(\"partOfSpeech\")\n",
    "            examples = d.get(\"examples\", [])\n",
    "            results.append({\"pos\": pos, \"definition\": text, \"examples\": examples})\n",
    "\n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"found\": bool(results),\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e7bfe166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_tagalog_definition(word):\n",
    "    # Configure headless Chrome\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        url = f\"https://www.tagalog.com/dictionary/{word}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        # Give time for Cloudflare & JS rendering\n",
    "        time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # First div with class \"definition\"\n",
    "        first_def_div = soup.find(\"div\", class_=\"definition\")\n",
    "        if not first_def_div:\n",
    "            return None\n",
    "\n",
    "        # Third div inside it\n",
    "        inner_divs = first_def_div.find_all(\"div\")\n",
    "        if len(inner_divs) < 3:\n",
    "            return None\n",
    "\n",
    "        target_div = inner_divs[2]  # zero-indexed\n",
    "\n",
    "        # Remove <span> tags\n",
    "        for span in target_div.find_all(\"span\"):\n",
    "            span.unwrap()\n",
    "\n",
    "        return target_div.get_text(strip=True)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "print(get_tagalog_definition(\"makikislap\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9e1f9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Result 1: salamat\n",
      "Definition 1: [expression]thank you•thanks18 Example Sentences\n",
      "\n",
      "Search Result 2: matsala\n",
      "Definition 2: [slang]thank you•* slang derived from the word \"salamat\"(root: salamat)\n",
      "\n",
      "Search Result 3: lamat\n",
      "Definition 3: [noun]crack•glassware\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_tagalog_search_and_definitions(word):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        url = f\"https://www.tagalog.com/#{word}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(3)  # wait for JS & Cloudflare\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            result_id = f\"search_result{i}\"\n",
    "            result_elem = soup.find(id=result_id)\n",
    "            if not result_elem:\n",
    "                continue\n",
    "\n",
    "            # Clean search result inner HTML (unwrap spans)\n",
    "            for span in result_elem.find_all(\"span\"):\n",
    "                span.unwrap()\n",
    "            search_result_text = result_elem.get_text(strip=True)\n",
    "\n",
    "            # First sibling div with definitions\n",
    "            def_div = result_elem.find_next_sibling(\"div\")\n",
    "            if def_div:\n",
    "                for span in def_div.find_all(\"span\"):\n",
    "                    span.unwrap()\n",
    "                def_text = def_div.get_text(strip=True)\n",
    "            else:\n",
    "                def_text = None\n",
    "\n",
    "            results.append({\n",
    "                \"search_result\": search_result_text,\n",
    "                \"definition\": def_text\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    results = get_tagalog_search_and_definitions(\"salamat\")\n",
    "    for idx, res in enumerate(results, 1):\n",
    "        print(f\"Search Result {idx}: {res['search_result']}\")\n",
    "        print(f\"Definition {idx}: {res['definition']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
